#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
çŸ¥è¯†å›¾è°±æ„å»º MCP æœåŠ¡å™¨ - å¢å¼ºç‰ˆ
æä¾›å…¨è‡ªåŠ¨åŒ–çš„çŸ¥è¯†å›¾è°±æ„å»ºæœåŠ¡ + é«˜çº§åˆ†æåŠŸèƒ½
"""

import asyncio
import json
import time
import sys
import os
from typing import Any, Sequence, Dict
from dotenv import load_dotenv
from dataclasses import asdict

# åŠ è½½ç¯å¢ƒå˜é‡
load_dotenv()

# è®¾ç½®æ§åˆ¶å°ç¼–ç 
if sys.platform == "win32":
    os.system("chcp 65001 > nul")
    sys.stdout.reconfigure(encoding='utf-8')
    sys.stderr.reconfigure(encoding='utf-8')

from mcp.server import NotificationOptions, Server
from mcp.server.models import InitializationOptions
from mcp.server.stdio import stdio_server
from mcp.types import (
    Resource,
    Tool,
    TextContent,
    ImageContent,
    EmbeddedResource,
    LoggingLevel
)
from dataclasses import asdict # ç¡®ä¿ asdict è¢«å¯¼å…¥

# åŸæœ‰æ¨¡å—
from data_quality import DataQualityAssessor
from knowledge_completion import KnowledgeCompletor
from kg_utils import KnowledgeGraphBuilder
from kg_visualizer import KnowledgeGraphVisualizer

# æ–°å¢åˆ†ææ¨¡å—
try:
    from content_enhancement.analysis_pipeline import analyze_knowledge_graph, AnalysisConfig
    from content_enhancement.enhancement_executor import EnhancementExecutor
    from kg_utils import Triple # ç¡®ä¿ Triple è¢«å¯¼å…¥
    ANALYSIS_AVAILABLE = True
    print("åˆ†ææ¨¡å—åŠ è½½æˆåŠŸ")
except ImportError as e:
    ANALYSIS_AVAILABLE = False
    print(f"åˆ†ææ¨¡å—åŠ è½½å¤±è´¥: {e}")
    print("é«˜çº§åˆ†æåŠŸèƒ½å°†ä¸å¯ç”¨")
except Exception as e:
    ANALYSIS_AVAILABLE = False
    print(f"åˆ†ææ¨¡å—åˆå§‹åŒ–é”™è¯¯: {e}")
    print("é«˜çº§åˆ†æåŠŸèƒ½å°†ä¸å¯ç”¨")

# å…¨å±€ç»„ä»¶
try:
    quality_assessor = DataQualityAssessor()
    knowledge_completor = KnowledgeCompletor()
    kg_builder = KnowledgeGraphBuilder(api_key=os.getenv("OPENAI_API_KEY"))
    kg_visualizer = KnowledgeGraphVisualizer()
    if ANALYSIS_AVAILABLE:
        enhancement_executor = EnhancementExecutor()
    print("æ ¸å¿ƒç»„ä»¶åˆå§‹åŒ–æˆåŠŸ")
except Exception as e:
    print(f"æ ¸å¿ƒç»„ä»¶åˆå§‹åŒ–å¤±è´¥: {e}")
    raise

# åˆ›å»ºæœåŠ¡å™¨å®ä¾‹
server = Server("knowledge-graph-builder-enhanced")


@server.list_tools()
async def handle_list_tools() -> list[Tool]:
    """
    åˆ—å‡ºå¯ç”¨çš„å·¥å…·
    """
    # ç›´æ¥æ„å»ºçŸ¥è¯†å›¾è°±ä¸è¿›è¡Œå¢å¼ºï¼Œç”¨äºå¯¹æ¯”å®éªŒ
    tools = [
        Tool(
            name="build_knowledge_graph",
            description="æ„å»ºçŸ¥è¯†å›¾è°±ï¼šç›´æ¥ä»æ–‡æœ¬æå–å®ä½“ä¸å…³ç³»å¹¶ç”Ÿæˆå¯è§†åŒ–ï¼ˆä¸è¿›è¡Œå†…å®¹å¢å¼ºï¼‰",
            inputSchema={
                "type": "object",
                "properties": {
                    "text": {
                        "type": "string",
                        "description": "è¦å¤„ç†çš„æ–‡æœ¬æ•°æ®"
                    },
                    "output_file": {
                        "type": "string",
                        "description": "å¯è§†åŒ–è¾“å‡ºæ–‡ä»¶åï¼ˆå¯é€‰ï¼‰",
                        "default": "knowledge_graph.html"
                    }
                },
                "required": ["text"]
            }
        )
    ]
    
    # å¦‚æœåˆ†ææ¨¡å—å¯ç”¨ï¼Œæ·»åŠ é«˜çº§åˆ†æå·¥å…·ï¼Œå¯¹æ–‡æœ¬è¿›è¡Œåˆ†æï¼Œæä¾›é«˜è´¨é‡è¯„ä¼°å’Œæ”¹è¿›å»ºè®®
    if ANALYSIS_AVAILABLE:
        tools.append(
            Tool(
                name="analyze_knowledge_graph",
                description="é«˜çº§çŸ¥è¯†å›¾è°±åˆ†æï¼šå…¨å±€åˆ†æ+ç»†èŠ‚åˆ†æï¼Œæä¾›è´¨é‡è¯„ä¼°å’Œæ”¹è¿›å»ºè®®",
                inputSchema={
                    "type": "object",
                    "properties": {
                        "text": {
                            "type": "string",
                            "description": "è¦åˆ†æçš„æ–‡æœ¬æ•°æ®"
                        },
                        "enable_global_analysis": {
                            "type": "boolean",
                            "description": "å¯ç”¨å…¨å±€åˆ†æ",
                            "default": True
                        },
                        "enable_detail_analysis": {
                            "type": "boolean",
                            "description": "å¯ç”¨ç»†èŠ‚åˆ†æ",
                            "default": True
                        },
                        "similarity_threshold": {
                            "type": "number",
                            "description": "ç›¸ä¼¼åº¦é˜ˆå€¼",
                            "default": 0.3
                        },
                        "max_recommendations": {
                            "type": "integer",
                            "description": "æœ€å¤§å»ºè®®æ•°é‡",
                            "default": 15
                        }
                    },
                    "required": ["text"]
                }
            )
        )
        
        tools.append(
            Tool(
                name="build_and_analyze_kg",
                description="æ„å»ºçŸ¥è¯†å›¾è°±å¹¶è¿›è¡Œé«˜çº§åˆ†æï¼šç»“åˆæ„å»ºå’Œåˆ†æåŠŸèƒ½çš„ä¸€ä½“åŒ–å·¥å…·ï¼Œæ”¯æŒè‡ªåŠ¨å¢å¼º",
                inputSchema={
                    "type": "object",
                    "properties": {
                        "text": {
                            "type": "string",
                            "description": "è¦å¤„ç†çš„æ–‡æœ¬æ•°æ®"
                        },
                        "output_file": {
                            "type": "string",
                            "description": "å¯è§†åŒ–è¾“å‡ºæ–‡ä»¶åï¼ˆå¯é€‰ï¼‰",
                            "default": "enhanced_knowledge_graph.html"
                        },
                        "enable_analysis": {
                            "type": "boolean",
                            "description": "å¯ç”¨é«˜çº§åˆ†æ",
                            "default": True
                        },
                        "auto_enhance": {
                            "type": "boolean",
                            "description": "æ˜¯å¦è‡ªåŠ¨å¢å¼ºçŸ¥è¯†å›¾è°±",
                            "default": False
                        }
                    },
                    "required": ["text"]
                }
            )
        )
        
        tools.append(
            Tool(
                name="process_text_file_to_cypher",
                description="æ‰¹é‡å¤„ç†æ–‡æœ¬æ–‡ä»¶ï¼Œæå–ä¸‰å…ƒç»„å¹¶ç”Ÿæˆ Neo4j Cypher è„šæœ¬",
                inputSchema={
                    "type": "object",
                    "properties": {
                        "input_file": {
                            "type": "string",
                            "description": "åŒ…å«å¾…å¤„ç†æ–‡æœ¬çš„è¾“å…¥æ–‡ä»¶è·¯å¾„ï¼ˆ.txtï¼‰"
                        },
                        "output_file": {
                            "type": "string",
                            "description": "Cypherè„šæœ¬çš„è¾“å‡ºæ–‡ä»¶åï¼ˆå¯é€‰ï¼‰",
                            "default": "neo4j_import.cypher"
                        }
                    },
                    "required": ["input_file"]
                }
            )
        )
    
    return tools


@server.call_tool()
async def handle_call_tool(name: str, arguments: dict[str, Any]) -> list[TextContent]:
    """
    å¤„ç†å·¥å…·è°ƒç”¨
    """
    if name == "build_knowledge_graph":
        return await build_knowledge_graph_tool(arguments)
    elif name == "analyze_knowledge_graph" and ANALYSIS_AVAILABLE:
        return await analyze_knowledge_graph_tool(arguments)
    elif name == "build_and_analyze_kg" and ANALYSIS_AVAILABLE:
        return await build_and_analyze_kg_tool(arguments)
    elif name == "process_text_file_to_cypher" and ANALYSIS_AVAILABLE:
        return await process_text_file_to_cypher_tool(arguments)
    else:
        raise ValueError(f"æœªçŸ¥å·¥å…·: {name}")


async def build_knowledge_graph_tool(arguments: dict[str, Any]) -> list[TextContent]:
    """
    æ„å»ºçŸ¥è¯†å›¾è°±ï¼ˆä¸è¿›è¡Œè´¨é‡è¯„ä¼°ã€çŸ¥è¯†è¡¥å…¨æˆ–å…¶ä»–å†…å®¹å¢å¼ºï¼‰
    """
    try:
        text = arguments.get("text", "")
        output_file = arguments.get("output_file", "knowledge_graph.html")

        if not text.strip():
            return [TextContent(
                type="text",
                text=json.dumps({
                    "success": False,
                    "error": "è¾“å…¥æ–‡æœ¬ä¸èƒ½ä¸ºç©º"
                }, ensure_ascii=False, indent=2)
            )]

        start_time = time.time()

        # ç›´æ¥æ„å»ºçŸ¥è¯†å›¾è°±
        kg_result = await kg_builder.build_graph(text, use_llm=True)

        # æ£€æŸ¥æ˜¯å¦æˆåŠŸæå–åˆ°å®ä½“å’Œä¸‰å…ƒç»„
        if not kg_result["entities"] and not kg_result["triples"]:
            return [TextContent(
                type="text",
                text=json.dumps({
                    "success": False,
                    "error": "æ— æ³•ä»è¾“å…¥æ–‡æœ¬ä¸­æå–åˆ°æœ‰æ•ˆçš„å®ä½“æˆ–å…³ç³»",
                    "suggestion": "è¯·å°è¯•è¾“å…¥åŒ…å«æ˜ç¡®å®ä½“å’Œå…³ç³»çš„æ–‡æœ¬"
                }, ensure_ascii=False, indent=2)
            )]

        # ç”Ÿæˆå¯è§†åŒ–
        visualization_file = kg_visualizer.save_simple_visualization(
            kg_result["triples"],
            kg_result["entities"],
            kg_result["relations"],
            output_file
        )

        abs_path = os.path.abspath(visualization_file)
        visualization_url = f"file:///{abs_path.replace(os.sep, '/')}"
        http_url = f"http://localhost:8000/{visualization_file}"
        server_info = f"å¯æ‰‹åŠ¨å¯åŠ¨HTTPæœåŠ¡å™¨è®¿é—®ï¼šåœ¨é¡¹ç›®ç›®å½•è¿è¡Œ 'python -m http.server 8000'ï¼Œç„¶åè®¿é—® {http_url}"

        processing_time = time.time() - start_time

        # æ„å»ºç»“æœ
        result = {
            "success": True,
            "input_text": text,
            "processing_time": round(processing_time, 3),
            "knowledge_graph": {
                "entities_count": len(kg_result["entities"]),
                "relations_count": len(kg_result["relations"]),
                "triples_count": len(kg_result["triples"]),
                "entities": kg_result["entities"],
                "relations": kg_result["relations"]
            },
            "visualization": {
                "file_path": visualization_file,
                "file_url": visualization_url,
                "http_url": http_url,
                "server_info": server_info
            }
        }

        return [TextContent(
            type="text",
            text=json.dumps(result, ensure_ascii=False, indent=2)
        )]

    except Exception as e:
        import traceback
        error_details = traceback.format_exc()
        return [TextContent(
            type="text",
            text=json.dumps({
                "success": False,
                "error": str(e),
                "error_details": error_details
            }, ensure_ascii=False, indent=2)
        )]


def _generate_cypher_script(triples: Sequence[Dict[str, Any]]) -> str:
    """å°†ä¸‰å…ƒç»„å­—å…¸åˆ—è¡¨è½¬æ¢ä¸º Neo4j Cypher MERGE è¯­å¥"""
    if not triples:
        return "# æ— å¯å¯¼å…¥çš„ä¸‰å…ƒç»„ã€‚\n"

    nodes = set()
    for t in triples:
        nodes.add(t['head'])
        nodes.add(t['tail'])

    cypher_statements = []

    # MERGE nodes
    cypher_statements.append("// --- 1. åˆ›å»ºæˆ–åŒ¹é…èŠ‚ç‚¹ ---")
    for node in sorted(list(nodes)):
        # æ­£ç¡®å¤„ç†å¸¦å¼•å·çš„èŠ‚ç‚¹åç§°
        node_escaped = node.replace("'", "\\'")
        cypher_statements.append(f"MERGE (:`Entity` {{name: '{node_escaped}'}});")

    # åˆ›å»ºç´¢å¼•ä»¥åŠ é€Ÿåˆå¹¶
    cypher_statements.append("\n// --- 2. åˆ›å»ºç´¢å¼•ä»¥åŠ é€Ÿ ---")
    cypher_statements.append("CREATE INDEX IF NOT EXISTS FOR (n:Entity) ON (n.name);")
    
    # MERGE relationships
    cypher_statements.append("\n// --- 3. åˆ›å»ºæˆ–åŒ¹é…å…³ç³» ---")
    for t in triples:
        head = t['head'].replace("'", "\\'")
        tail = t['tail'].replace("'", "\\'")
        # å°†å…³ç³»ä¸­çš„éå­—æ¯æ•°å­—å­—ç¬¦æ›¿æ¢ä¸ºä¸‹åˆ’çº¿ï¼Œä»¥ç¬¦åˆCypheræ ‡å‡†
        relation_type = ''.join(c if c.isalnum() else '_' for c in t['relation']).upper()
        if not relation_type:  # é¿å…ç©ºå…³ç³»ç±»å‹
            relation_type = "RELATED_TO"
        
        cypher_statements.append(
            f"MATCH (h:`Entity` {{name: '{head}'}}), (t:`Entity` {{name: '{tail}'}}) "
            f"MERGE (h)-[:`{relation_type}`]->(t);"
        )

    return "\n".join(cypher_statements)


async def process_text_file_to_cypher_tool(arguments: dict[str, Any]) -> list[TextContent]:
    """
    æ‰¹é‡å¤„ç†æ–‡æœ¬æ–‡ä»¶å¹¶ç”Ÿæˆ Cypher è„šæœ¬çš„å·¥å…·
    """
    try:
        input_file = arguments.get("input_file")
        output_file = arguments.get("output_file", "neo4j_import.cypher")

        if not input_file or not os.path.exists(input_file):
            return [TextContent(text=json.dumps({"success": False, "error": f"è¾“å…¥æ–‡ä»¶ä¸å­˜åœ¨: {input_file}"}, ensure_ascii=False))]

        with open(input_file, 'r', encoding='utf-8') as f:
            lines = [line.strip().split('\t', 1)[-1] for line in f if line.strip()]

        if not lines:
            return [TextContent(text=json.dumps({"success": False, "error": "è¾“å…¥æ–‡ä»¶ä¸ºç©ºæˆ–æ ¼å¼ä¸æ­£ç¡®"}, ensure_ascii=False))]

        start_time = time.time()
        
        # å¹¶å‘å¤„ç†æ¯ä¸€è¡Œ
        tasks = [build_and_analyze_kg_tool({"text": line, "auto_enhance": True, "output_file": "off"}) for line in lines]
        results = await asyncio.gather(*tasks, return_exceptions=True)

        all_triples = []
        processed_lines = 0
        failed_lines = 0

        for res in results:
            if isinstance(res, Exception) or not res:
                failed_lines += 1
                continue
            
            try:
                data = json.loads(res[0].text)
                if data.get("success"):
                    processed_lines += 1
                    # ä» summary ä¸­æå– final_triples
                    summary = data.get("summary", {})
                    triples_from_summary = summary.get("final_triples", [])
                    if triples_from_summary:
                        all_triples.extend(triples_from_summary)
                else:
                    failed_lines += 1
            except (json.JSONDecodeError, IndexError):
                failed_lines += 1

        # ä½¿ç”¨å»é‡ç¡®ä¿ä¸‰å…ƒç»„å”¯ä¸€æ€§
        unique_triples = [dict(t) for t in {tuple(d.items()) for d in all_triples}]

        cypher_script = _generate_cypher_script(unique_triples)
        
        with open(output_file, 'w', encoding='utf-8') as f:
            f.write(cypher_script)

        processing_time = time.time() - start_time

        result_summary = {
            "success": True,
            "processing_time": round(processing_time, 3),
            "total_lines": len(lines),
            "processed_lines": processed_lines,
            "failed_lines": failed_lines,
            "total_triples_generated": len(unique_triples),
            "cypher_script_file": os.path.abspath(output_file)
        }
        
        return [TextContent(text=json.dumps(result_summary, ensure_ascii=False, indent=2))]

    except Exception as e:
        import traceback
        return [TextContent(text=json.dumps({
            "success": False, 
            "error": str(e),
            "error_details": traceback.format_exc()
        }, ensure_ascii=False, indent=2))]


async def analyze_knowledge_graph_tool(arguments: dict[str, Any]) -> list[TextContent]:
    """
    æ–°å¢çš„çŸ¥è¯†å›¾è°±é«˜çº§åˆ†æå·¥å…·
    """
    try:
        text = arguments.get("text", "")
        enable_global = arguments.get("enable_global_analysis", True)
        enable_detail = arguments.get("enable_detail_analysis", True)
        similarity_threshold = arguments.get("similarity_threshold", 0.3)
        max_recommendations = arguments.get("max_recommendations", 15)

        if not text.strip():
            return [TextContent(
                type="text",
                text=json.dumps({
                    "success": False,
                    "error": "è¾“å…¥æ–‡æœ¬ä¸èƒ½ä¸ºç©º"
                }, ensure_ascii=False, indent=2)
            )]

        start_time = time.time()

        # é¦–å…ˆæ„å»ºåŸºç¡€çŸ¥è¯†å›¾è°±
        kg_result = await kg_builder.build_graph(text, use_llm=True)

        if not kg_result["entities"] and not kg_result["triples"]:
            return [TextContent(
                type="text",
                text=json.dumps({
                    "success": False,
                    "error": "æ— æ³•ä»è¾“å…¥æ–‡æœ¬ä¸­æå–åˆ°æœ‰æ•ˆçš„å®ä½“æˆ–å…³ç³»"
                }, ensure_ascii=False, indent=2)
            )]

        # è½¬æ¢æ•°æ®æ ¼å¼ç”¨äºåˆ†æ
        entities = [
            {
                'name': entity,
                'type': 'unknown',  # å¯ä»¥æ ¹æ®éœ€è¦æ”¹è¿›ç±»å‹æ¨æ–­
                'attributes': {},
                'relations': []
            }
            for entity in kg_result["entities"]
        ]

        relations = [
            {
                'name': triple.relation,
                'source': triple.head,
                'target': triple.tail,
                'type': 'unknown'
            }
            for triple in kg_result["triples"]
        ]

        # é…ç½®åˆ†æå‚æ•°
        config = AnalysisConfig(
            enable_global_analysis=enable_global,
            enable_detail_analysis=enable_detail,
            similarity_threshold=similarity_threshold,
            max_recommendations=max_recommendations
        )

        # æ‰§è¡Œé«˜çº§åˆ†æ
        analysis_result = await analyze_knowledge_graph(
            text, entities, relations, config
        )

        processing_time = time.time() - start_time

        # æ„å»ºç»“æœ
        result = {
            "success": True,
            "input_text": text,
            "processing_time": round(processing_time, 3),
            "knowledge_graph": {
                "entities_count": len(kg_result["entities"]),
                "relations_count": len(kg_result["relations"]),
                "triples_count": len(kg_result["triples"]),
                "entities": kg_result["entities"],
                "relations": kg_result["relations"]
            },
            "analysis_results": {
                "llm_status": analysis_result.llm_status,
                "timestamp": analysis_result.timestamp,
                "quality_score": analysis_result.quality_metrics.get('overall_score', 0),
                "total_issues": analysis_result.quality_metrics.get('issue_count', 0),
                "critical_issues": analysis_result.quality_metrics.get('critical_issues', 0),
                "recommendations_count": len(analysis_result.integrated_recommendations),
                "top_recommendations": analysis_result.integrated_recommendations[:5]
            },
            "detailed_analysis": {
                "global_analysis": analysis_result.global_analysis_results is not None,
                "detail_analysis": analysis_result.detail_analysis_results is not None,
                "all_recommendations": analysis_result.integrated_recommendations
            }
        }

        return [TextContent(
            type="text",
            text=json.dumps(result, ensure_ascii=False, indent=2)
        )]

    except Exception as e:
        import traceback
        error_details = traceback.format_exc()
        return [TextContent(
            type="text",
            text=json.dumps({
                "success": False,
                "error": str(e),
                "error_details": error_details
            }, ensure_ascii=False, indent=2)
        )]


async def build_and_analyze_kg_tool(arguments: dict[str, Any]) -> list[TextContent]:
    """
    ä¸€ä½“åŒ–å·¥å…·ï¼šæ„å»ºçŸ¥è¯†å›¾è°±ã€åˆ†æã€è‡ªåŠ¨å¢å¼ºå¹¶ç”Ÿæˆå¯è§†åŒ–
    """
    try:
        text = arguments.get("text", "")
        output_file = arguments.get("output_file", "enhanced_knowledge_graph.html")
        enable_analysis = arguments.get("enable_analysis", True)
        auto_enhance = arguments.get("auto_enhance", True)

        if not text.strip():
            return [TextContent(
                type="text",
                text=json.dumps({
                    "success": False,
                    "error": "è¾“å…¥æ–‡æœ¬ä¸èƒ½ä¸ºç©º"
                }, ensure_ascii=False, indent=2)
            )]

        start_time = time.time()

        # é˜¶æ®µ1ï¼šæ•°æ®è´¨é‡è¯„ä¼°
        quality_result = await quality_assessor.assess_quality(text)

        # é˜¶æ®µ2ï¼šçŸ¥è¯†è¡¥å…¨ï¼ˆå¦‚æœéœ€è¦ï¼‰
        processed_text = text
        completion_info = {"skipped": True, "reason": "æ•°æ®è´¨é‡è‰¯å¥½"}

        if not quality_result["is_high_quality"]:
            completion_result = await knowledge_completor.complete_knowledge(text, quality_result)
            processed_text = completion_result["enhanced_data"]
            completion_info = {
                "skipped": False,
                "completions": completion_result["completions"],
                "corrections": completion_result["corrections"],
                "confidence": completion_result["confidence"]
            }

        # é˜¶æ®µ3ï¼šçŸ¥è¯†å›¾è°±æ„å»º
        kg_result = await kg_builder.build_graph(processed_text, use_llm=True)

        if not kg_result["entities"] and not kg_result["triples"]:
            return [TextContent(
                type="text",
                text=json.dumps({
                    "success": False,
                    "error": "æ— æ³•ä»è¾“å…¥æ–‡æœ¬ä¸­æå–åˆ°æœ‰æ•ˆçš„å®ä½“æˆ–å…³ç³»"
                }, ensure_ascii=False, indent=2)
            )]

        # é˜¶æ®µ4ï¼šé«˜çº§åˆ†æï¼ˆå¦‚æœå¯ç”¨ï¼‰
        analysis_result = None
        if enable_analysis:
            # è½¬æ¢æ•°æ®æ ¼å¼ç”¨äºåˆ†æ
            entities = [
                {
                    'name': entity,
                    'type': 'unknown',
                    'attributes': {},
                    'relations': []
                }
                for entity in kg_result["entities"]
            ]

            relations = [
                {
                    'name': triple.relation,
                    'source': triple.head,
                    'target': triple.tail,
                    'type': 'unknown'
                }
                for triple in kg_result["triples"]
            ]

            # é…ç½®åˆ†æå‚æ•°
            config = AnalysisConfig(
                enable_global_analysis=True,
                enable_detail_analysis=True,
                similarity_threshold=0.3,
                max_recommendations=10
            )

            # æ‰§è¡Œé«˜çº§åˆ†æ
            analysis_result = await analyze_knowledge_graph(
                processed_text, entities, relations, config
            )

        # é˜¶æ®µ5ï¼šè‡ªåŠ¨å¢å¼ºï¼ˆå¦‚æœå¯ç”¨ï¼‰
        enhancement_result = None
        final_entities = kg_result["entities"]
        final_relations = kg_result["relations"]
        final_triples = kg_result["triples"]

        if auto_enhance and analysis_result:
            # ç¡®ä¿ä¼ é€’çš„æ˜¯ç»“æ„åŒ–çš„å®ä½“å’Œå…³ç³»åˆ—è¡¨
            enhancement_result = await enhancement_executor.execute_enhancements(
                processed_text, entities, relations, analysis_result
            )

            # ä½¿ç”¨å¢å¼ºåçš„æ•°æ®
            final_entities = [e['name'] for e in enhancement_result.enhanced_entities]
            final_relations = [r['name'] for r in enhancement_result.enhanced_relations]

            # é‡æ–°è®¡ç®—æ‘˜è¦ä»¥è·å¾—å‡†ç¡®çš„â€œåŸå§‹â€ä¸â€œå¢å¼ºâ€å¯¹æ¯”
            enhancement_summary = {
                'status': 'Completed',
                'original_entity_count': len(kg_result["entities"]),
                'enhanced_entity_count': len(final_entities),
                'original_relation_count': len(kg_result["triples"]),
                'enhanced_relation_count': len(enhancement_result.enhanced_triples),
                'applied_enhancements_count': len(enhancement_result.applied_enhancements)
            }
            enhancement_result.enhancement_summary = enhancement_summary


            # æ„å»ºå¢å¼ºåçš„ä¸‰å…ƒç»„ç”¨äºå¯è§†åŒ–
            enhanced_triples = []
            # Triple å¯¹è±¡æ˜¯ kg_utils ä¸­å®šä¹‰çš„ï¼Œéœ€è¦ä» enhanced_triples (dict) è½¬æ¢
            from kg_utils import Triple
            for triple_dict in enhancement_result.enhanced_triples:
                enhanced_triple = Triple(
                    head=triple_dict['head'],
                    relation=triple_dict['relation'],
                    tail=triple_dict['tail'],
                    confidence=triple_dict.get('confidence', 0.8)
                )
                enhanced_triples.append(enhanced_triple)

            final_triples = enhanced_triples

        # é˜¶æ®µ6ï¼šç”Ÿæˆå¯è§†åŒ–
        if output_file != "off":
            visualization_file = kg_visualizer.save_simple_visualization(
                final_triples,
                final_entities,
                final_relations,
                output_file
            )
            abs_path = os.path.abspath(visualization_file)
            visualization_url = f"file:///{abs_path.replace(os.sep, '/')}"
            http_url = f"http://localhost:8000/{visualization_file}"
            viz_info = {
                "file_path": visualization_file,
                "file_url": visualization_url,
                "http_url": http_url,
                "server_info": f"å¯æ‰‹åŠ¨å¯åŠ¨HTTPæœåŠ¡å™¨è®¿é—®ï¼šåœ¨é¡¹ç›®ç›®å½•è¿è¡Œ 'python -m http.server 8000'ï¼Œç„¶åè®¿é—® {http_url}"
            }
        else:
            visualization_file = "off"
            visualization_url = "off"
            viz_info = {"status": "Visualization disabled"}


        processing_time = time.time() - start_time

        # æ„å»ºç»“æœ
        result = {
            "success": True,
            "input_text": text,
            "processing_time": round(processing_time, 3),
            "stages": {
                "quality_assessment": {
                    "quality_score": quality_result["quality_score"],
                    "is_high_quality": quality_result["is_high_quality"],
                    "completeness": quality_result["completeness"],
                    "consistency": quality_result["consistency"],
                    "relevance": quality_result["relevance"],
                    "issues": quality_result["issues"],
                    "recommendation": quality_result["recommendation"]
                },
                "knowledge_completion": completion_info,
                "original_knowledge_graph": {
                    "entities_count": len(kg_result["entities"]),
                    "relations_count": len(kg_result["relations"]),
                    "triples_count": len(kg_result["triples"]),
                    "entities": kg_result["entities"],
                    "relations": kg_result["relations"],
                    "average_confidence": sum(kg_result["confidence_scores"]) / len(kg_result["confidence_scores"]) if kg_result["confidence_scores"] else 0
                },
                "analysis_results": {
                    "analysis_enabled": enable_analysis,
                    "llm_status": analysis_result.llm_status if analysis_result else "NOT_APPLICABLE",
                    "analysis_performed": analysis_result is not None,
                    "timestamp": analysis_result.timestamp if analysis_result else None,
                    "quality_score": analysis_result.quality_metrics.get('overall_score', 0) if analysis_result else None,
                    "total_issues": analysis_result.quality_metrics.get('issue_count', 0) if analysis_result else 0,
                    "critical_issues": analysis_result.quality_metrics.get('critical_issues', 0) if analysis_result else 0,
                    "recommendations_count": len(analysis_result.integrated_recommendations) if analysis_result else 0,
                    "top_recommendations": analysis_result.integrated_recommendations[:5] if analysis_result else []
                },
                "enhancement_results": {
                    "auto_enhance_enabled": auto_enhance,
                    "enhancement_applied": enhancement_result is not None,
                    "enhancement_summary": enhancement_result.enhancement_summary if enhancement_result else None,
                    "applied_enhancements": enhancement_result.applied_enhancements if enhancement_result else [],
                    "final_entities_count": len(final_entities),
                    "final_relations_count": len(final_relations),
                    "final_triples_count": len(final_triples)
                },
                "visualization": viz_info
            },
            "summary": {
                "original_text": text,
                "processed_text": processed_text,
                "quality_improved": not quality_result["is_high_quality"],
                "analysis_performed": analysis_result is not None,
                "enhancement_applied": enhancement_result is not None,
                "final_entities": len(final_entities),
                "final_relations": len(final_relations),
                "final_triples": [asdict(t) for t in final_triples],
                "visualization_ready": visualization_file != "off",
                "visualization_file": visualization_file,
                "visualization_url": visualization_url
            }
        }

        return [TextContent(
            type="text",
            text=json.dumps(result, ensure_ascii=False, indent=2)
        )]

    except Exception as e:
        import traceback
        error_details = traceback.format_exc()
        return [TextContent(
            type="text",
            text=json.dumps({
                "success": False,
                "error": str(e),
                "error_details": error_details
            }, ensure_ascii=False, indent=2)
        )]


async def main():
    """
    è¿è¡ŒæœåŠ¡å™¨
    """
    print("ğŸš€ å¯åŠ¨çŸ¥è¯†å›¾è°±æ„å»ºæœåŠ¡å™¨ï¼ˆå¢å¼ºç‰ˆï¼‰")
    print(f"ğŸ”§ é«˜çº§åˆ†æåŠŸèƒ½: {'âœ… å¯ç”¨' if ANALYSIS_AVAILABLE else 'âŒ ä¸å¯ç”¨'}")
    
    try:
        # ç¡®ä¿æ‰€æœ‰ç»„ä»¶éƒ½æ­£å¸¸åˆå§‹åŒ–
        print("ğŸ”§ éªŒè¯ç»„ä»¶çŠ¶æ€...")
        if not hasattr(quality_assessor, 'assess_quality'):
            raise RuntimeError("è´¨é‡è¯„ä¼°å™¨æœªæ­£ç¡®åˆå§‹åŒ–")
        if not hasattr(kg_builder, 'build_graph'):
            raise RuntimeError("çŸ¥è¯†å›¾è°±æ„å»ºå™¨æœªæ­£ç¡®åˆå§‹åŒ–")
        print("âœ… æ‰€æœ‰ç»„ä»¶éªŒè¯é€šè¿‡")
        
        # ä½¿ç”¨ stdio ä¼ è¾“è¿è¡ŒæœåŠ¡å™¨
        print("ğŸ”— å¯åŠ¨MCPæœåŠ¡å™¨...")
        
        # åˆ›å»ºåˆå§‹åŒ–é€‰é¡¹
        init_options = InitializationOptions(
            server_name="knowledge-graph-builder-enhanced",
            server_version="2.0.0",
            capabilities=server.get_capabilities(
                notification_options=NotificationOptions(),
                experimental_capabilities={}
            ),
            timeoutSeconds=300  # å°†é»˜è®¤è¶…æ—¶ä»60ç§’å»¶é•¿åˆ°300ç§’
        )
        
        async with stdio_server() as (read_stream, write_stream):
            # ç¡®ä¿æœåŠ¡å™¨å®Œå…¨åˆå§‹åŒ–
            await asyncio.sleep(3)  # å»¶é•¿å»¶è¿Ÿï¼Œé¿å…å‰ç«¯è¿‡æ—©å‘é€è¯·æ±‚
            
            print("âœ… å¼€å§‹è¿è¡ŒæœåŠ¡å™¨...")
            await server.run(
                read_stream,
                write_stream,
                init_options,
            )
            
    except KeyboardInterrupt:
        print("\nğŸ›‘ æœåŠ¡å™¨æ”¶åˆ°ä¸­æ–­ä¿¡å·ï¼Œæ­£åœ¨å…³é—­...")
    except Exception as e:
        print(f"âŒ æœåŠ¡å™¨å¯åŠ¨å¤±è´¥: {e}")
        import traceback
        traceback.print_exc()
        raise
    finally:
        print("ğŸ”š æœåŠ¡å™¨å·²å…³é—­")


if __name__ == "__main__":
    asyncio.run(main()) 