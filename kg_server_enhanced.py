#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
çŸ¥è¯†å›¾è°±æ„å»º MCP æœåŠ¡å™¨ - å¢å¼ºç‰ˆ
æä¾›å…¨è‡ªåŠ¨åŒ–çš„çŸ¥è¯†å›¾è°±æ„å»ºæœåŠ¡ + é«˜çº§åˆ†æåŠŸèƒ½
"""

import asyncio
import json
import time
import sys
import os
from typing import Any, Sequence
from dotenv import load_dotenv

# åŠ è½½ç¯å¢ƒå˜é‡
load_dotenv()

# è®¾ç½®æ§åˆ¶å°ç¼–ç 
if sys.platform == "win32":
    os.system("chcp 65001 > nul")
    sys.stdout.reconfigure(encoding='utf-8')
    sys.stderr.reconfigure(encoding='utf-8')

from mcp.server import NotificationOptions, Server
from mcp.server.models import InitializationOptions
from mcp.server.stdio import stdio_server
from mcp.types import (
    Resource,
    Tool,
    TextContent,
    ImageContent,
    EmbeddedResource,
    LoggingLevel
)

# åŸæœ‰æ¨¡å—
from data_quality import DataQualityAssessor
from knowledge_completion import KnowledgeCompletor
from kg_utils import KnowledgeGraphBuilder
from kg_visualizer import KnowledgeGraphVisualizer

# æ–°å¢åˆ†ææ¨¡å—
try:
    from content_enhancement.analysis_pipeline import analyze_knowledge_graph, AnalysisConfig
    from content_enhancement.enhancement_executor import EnhancementExecutor
    ANALYSIS_AVAILABLE = True
    print("âœ… åˆ†ææ¨¡å—åŠ è½½æˆåŠŸ")
except ImportError as e:
    ANALYSIS_AVAILABLE = False
    print(f"âŒ åˆ†ææ¨¡å—åŠ è½½å¤±è´¥: {e}")
    print("é«˜çº§åˆ†æåŠŸèƒ½å°†ä¸å¯ç”¨")
except Exception as e:
    ANALYSIS_AVAILABLE = False
    print(f"âŒ åˆ†ææ¨¡å—åˆå§‹åŒ–é”™è¯¯: {e}")
    print("é«˜çº§åˆ†æåŠŸèƒ½å°†ä¸å¯ç”¨")

# å…¨å±€ç»„ä»¶
try:
    quality_assessor = DataQualityAssessor()
    knowledge_completor = KnowledgeCompletor()
    kg_builder = KnowledgeGraphBuilder(api_key=os.getenv("OPENAI_API_KEY"))
    kg_visualizer = KnowledgeGraphVisualizer()
    if ANALYSIS_AVAILABLE:
        enhancement_executor = EnhancementExecutor()
    print("âœ… æ ¸å¿ƒç»„ä»¶åˆå§‹åŒ–æˆåŠŸ")
except Exception as e:
    print(f"âŒ æ ¸å¿ƒç»„ä»¶åˆå§‹åŒ–å¤±è´¥: {e}")
    raise

# åˆ›å»ºæœåŠ¡å™¨å®ä¾‹
server = Server("knowledge-graph-builder-enhanced")


@server.list_tools()
async def handle_list_tools() -> list[Tool]:
    """
    åˆ—å‡ºå¯ç”¨çš„å·¥å…·
    """
    tools = [
        Tool(
            name="build_knowledge_graph",
            description="å…¨è‡ªåŠ¨æ„å»ºçŸ¥è¯†å›¾è°±ï¼šè‡ªåŠ¨è¯„ä¼°æ•°æ®è´¨é‡ã€è¡¥å…¨çŸ¥è¯†ã€æ„å»ºå›¾è°±å¹¶ç”Ÿæˆå¯è§†åŒ–",
            inputSchema={
                "type": "object",
                "properties": {
                    "text": {
                        "type": "string",
                        "description": "è¦å¤„ç†çš„æ–‡æœ¬æ•°æ®"
                    },
                    "output_file": {
                        "type": "string",
                        "description": "å¯è§†åŒ–è¾“å‡ºæ–‡ä»¶åï¼ˆå¯é€‰ï¼‰",
                        "default": "knowledge_graph.html"
                    }
                },
                "required": ["text"]
            }
        )
    ]
    
    # å¦‚æœåˆ†ææ¨¡å—å¯ç”¨ï¼Œæ·»åŠ é«˜çº§åˆ†æå·¥å…·
    if ANALYSIS_AVAILABLE:
        tools.append(
            Tool(
                name="analyze_knowledge_graph",
                description="é«˜çº§çŸ¥è¯†å›¾è°±åˆ†æï¼šå…¨å±€åˆ†æ+ç»†èŠ‚åˆ†æï¼Œæä¾›è´¨é‡è¯„ä¼°å’Œæ”¹è¿›å»ºè®®",
                inputSchema={
                    "type": "object",
                    "properties": {
                        "text": {
                            "type": "string",
                            "description": "è¦åˆ†æçš„æ–‡æœ¬æ•°æ®"
                        },
                        "enable_global_analysis": {
                            "type": "boolean",
                            "description": "å¯ç”¨å…¨å±€åˆ†æ",
                            "default": True
                        },
                        "enable_detail_analysis": {
                            "type": "boolean",
                            "description": "å¯ç”¨ç»†èŠ‚åˆ†æ",
                            "default": True
                        },
                        "similarity_threshold": {
                            "type": "number",
                            "description": "ç›¸ä¼¼åº¦é˜ˆå€¼",
                            "default": 0.3
                        },
                        "max_recommendations": {
                            "type": "integer",
                            "description": "æœ€å¤§å»ºè®®æ•°é‡",
                            "default": 15
                        }
                    },
                    "required": ["text"]
                }
            )
        )
        
        tools.append(
            Tool(
                name="build_and_analyze_kg",
                description="æ„å»ºçŸ¥è¯†å›¾è°±å¹¶è¿›è¡Œé«˜çº§åˆ†æï¼šç»“åˆæ„å»ºå’Œåˆ†æåŠŸèƒ½çš„ä¸€ä½“åŒ–å·¥å…·ï¼Œæ”¯æŒè‡ªåŠ¨å¢å¼º",
                inputSchema={
                    "type": "object",
                    "properties": {
                        "text": {
                            "type": "string",
                            "description": "è¦å¤„ç†çš„æ–‡æœ¬æ•°æ®"
                        },
                        "output_file": {
                            "type": "string",
                            "description": "å¯è§†åŒ–è¾“å‡ºæ–‡ä»¶åï¼ˆå¯é€‰ï¼‰",
                            "default": "enhanced_knowledge_graph.html"
                        },
                        "enable_analysis": {
                            "type": "boolean",
                            "description": "å¯ç”¨é«˜çº§åˆ†æ",
                            "default": True
                        },
                        "auto_enhance": {
                            "type": "boolean",
                            "description": "æ˜¯å¦è‡ªåŠ¨å¢å¼ºçŸ¥è¯†å›¾è°±",
                            "default": True
                        }
                    },
                    "required": ["text"]
                }
            )
        )
    
    return tools


@server.call_tool()
async def handle_call_tool(name: str, arguments: dict[str, Any]) -> list[TextContent]:
    """
    å¤„ç†å·¥å…·è°ƒç”¨
    """
    if name == "build_knowledge_graph":
        return await build_knowledge_graph_tool(arguments)
    elif name == "analyze_knowledge_graph" and ANALYSIS_AVAILABLE:
        return await analyze_knowledge_graph_tool(arguments)
    elif name == "build_and_analyze_kg" and ANALYSIS_AVAILABLE:
        return await build_and_analyze_kg_tool(arguments)
    else:
        raise ValueError(f"æœªçŸ¥å·¥å…·: {name}")


async def build_knowledge_graph_tool(arguments: dict[str, Any]) -> list[TextContent]:
    """
    åŸæœ‰çš„çŸ¥è¯†å›¾è°±æ„å»ºå·¥å…·ï¼ˆä¿æŒä¸å˜ï¼‰
    """
    try:
        text = arguments.get("text", "")
        output_file = arguments.get("output_file", "knowledge_graph.html")

        if not text.strip():
            return [TextContent(
                type="text",
                text=json.dumps({
                    "success": False,
                    "error": "è¾“å…¥æ–‡æœ¬ä¸èƒ½ä¸ºç©º"
                }, ensure_ascii=False, indent=2)
            )]

        start_time = time.time()

        # é˜¶æ®µ1ï¼šæ•°æ®è´¨é‡è¯„ä¼°
        quality_result = await quality_assessor.assess_quality(text)

        # é˜¶æ®µ2ï¼šçŸ¥è¯†è¡¥å…¨ï¼ˆå¦‚æœéœ€è¦ï¼‰
        processed_text = text
        completion_info = {"skipped": True, "reason": "æ•°æ®è´¨é‡è‰¯å¥½"}

        if not quality_result["is_high_quality"]:
            completion_result = await knowledge_completor.complete_knowledge(text, quality_result)
            processed_text = completion_result["enhanced_data"]
            completion_info = {
                "skipped": False,
                "completions": completion_result["completions"],
                "corrections": completion_result["corrections"],
                "confidence": completion_result["confidence"]
            }

        # é˜¶æ®µ3ï¼šçŸ¥è¯†å›¾è°±æ„å»º
        kg_result = await kg_builder.build_graph(processed_text, use_llm=True)

        # æ£€æŸ¥æ˜¯å¦æˆåŠŸæå–åˆ°å®ä½“å’Œä¸‰å…ƒç»„
        if not kg_result["entities"] and not kg_result["triples"]:
            return [TextContent(
                type="text",
                text=json.dumps({
                    "success": False,
                    "error": "æ— æ³•ä»è¾“å…¥æ–‡æœ¬ä¸­æå–åˆ°æœ‰æ•ˆçš„å®ä½“æˆ–å…³ç³»",
                    "suggestion": "è¯·å°è¯•è¾“å…¥åŒ…å«æ˜ç¡®å®ä½“å’Œå…³ç³»çš„æ–‡æœ¬"
                }, ensure_ascii=False, indent=2)
            )]

        # é˜¶æ®µ4ï¼šç”Ÿæˆå¯è§†åŒ–
        visualization_file = kg_visualizer.save_simple_visualization(
            kg_result["triples"],
            kg_result["entities"],
            kg_result["relations"],
            output_file
        )

        abs_path = os.path.abspath(visualization_file)
        visualization_url = f"file:///{abs_path.replace(os.sep, '/')}"
        http_url = f"http://localhost:8000/{visualization_file}"
        server_info = f"å¯æ‰‹åŠ¨å¯åŠ¨HTTPæœåŠ¡å™¨è®¿é—®ï¼šåœ¨é¡¹ç›®ç›®å½•è¿è¡Œ 'python -m http.server 8000'ï¼Œç„¶åè®¿é—® {http_url}"

        processing_time = time.time() - start_time

        # æ„å»ºç»“æœ
        result = {
            "success": True,
            "input_text": text,
            "processing_time": round(processing_time, 3),
            "stages": {
                "quality_assessment": {
                    "quality_score": quality_result["quality_score"],
                    "is_high_quality": quality_result["is_high_quality"],
                    "completeness": quality_result["completeness"],
                    "consistency": quality_result["consistency"],
                    "relevance": quality_result["relevance"],
                    "issues": quality_result["issues"],
                    "recommendation": quality_result["recommendation"]
                },
                "knowledge_completion": completion_info,
                "knowledge_graph": {
                    "entities_count": len(kg_result["entities"]),
                    "relations_count": len(kg_result["relations"]),
                    "triples_count": len(kg_result["triples"]),
                    "entities": kg_result["entities"],
                    "relations": kg_result["relations"],
                    "average_confidence": sum(kg_result["confidence_scores"]) / len(kg_result["confidence_scores"]) if kg_result["confidence_scores"] else 0
                },
                "visualization": {
                    "file_path": visualization_file,
                    "file_url": visualization_url,
                    "http_url": http_url,
                    "server_info": server_info
                }
            },
            "summary": {
                "original_text": text,
                "processed_text": processed_text,
                "quality_improved": not quality_result["is_high_quality"],
                "final_entities": len(kg_result["entities"]),
                "final_triples": len(kg_result["triples"]),
                "visualization_ready": True,
                "visualization_file": visualization_file,
                "visualization_url": visualization_url
            }
        }

        return [TextContent(
            type="text",
            text=json.dumps(result, ensure_ascii=False, indent=2)
        )]

    except Exception as e:
        import traceback
        error_details = traceback.format_exc()
        return [TextContent(
            type="text",
            text=json.dumps({
                "success": False,
                "error": str(e),
                "error_details": error_details
            }, ensure_ascii=False, indent=2)
        )]


async def analyze_knowledge_graph_tool(arguments: dict[str, Any]) -> list[TextContent]:
    """
    æ–°å¢çš„çŸ¥è¯†å›¾è°±é«˜çº§åˆ†æå·¥å…·
    """
    try:
        text = arguments.get("text", "")
        enable_global = arguments.get("enable_global_analysis", True)
        enable_detail = arguments.get("enable_detail_analysis", True)
        similarity_threshold = arguments.get("similarity_threshold", 0.3)
        max_recommendations = arguments.get("max_recommendations", 15)

        if not text.strip():
            return [TextContent(
                type="text",
                text=json.dumps({
                    "success": False,
                    "error": "è¾“å…¥æ–‡æœ¬ä¸èƒ½ä¸ºç©º"
                }, ensure_ascii=False, indent=2)
            )]

        start_time = time.time()

        # é¦–å…ˆæ„å»ºåŸºç¡€çŸ¥è¯†å›¾è°±
        kg_result = await kg_builder.build_graph(text, use_llm=True)

        if not kg_result["entities"] and not kg_result["triples"]:
            return [TextContent(
                type="text",
                text=json.dumps({
                    "success": False,
                    "error": "æ— æ³•ä»è¾“å…¥æ–‡æœ¬ä¸­æå–åˆ°æœ‰æ•ˆçš„å®ä½“æˆ–å…³ç³»"
                }, ensure_ascii=False, indent=2)
            )]

        # è½¬æ¢æ•°æ®æ ¼å¼ç”¨äºåˆ†æ
        entities = [
            {
                'name': entity,
                'type': 'unknown',  # å¯ä»¥æ ¹æ®éœ€è¦æ”¹è¿›ç±»å‹æ¨æ–­
                'attributes': {},
                'relations': []
            }
            for entity in kg_result["entities"]
        ]

        relations = [
            {
                'name': triple.relation,
                'source': triple.head,
                'target': triple.tail,
                'type': 'unknown'
            }
            for triple in kg_result["triples"]
        ]

        # é…ç½®åˆ†æå‚æ•°
        config = AnalysisConfig(
            enable_global_analysis=enable_global,
            enable_detail_analysis=enable_detail,
            similarity_threshold=similarity_threshold,
            max_recommendations=max_recommendations
        )

        # æ‰§è¡Œé«˜çº§åˆ†æ
        analysis_result = await analyze_knowledge_graph(
            text, entities, relations, config
        )

        processing_time = time.time() - start_time

        # æ„å»ºç»“æœ
        result = {
            "success": True,
            "input_text": text,
            "processing_time": round(processing_time, 3),
            "knowledge_graph": {
                "entities_count": len(kg_result["entities"]),
                "relations_count": len(kg_result["relations"]),
                "triples_count": len(kg_result["triples"]),
                "entities": kg_result["entities"],
                "relations": kg_result["relations"]
            },
            "analysis_results": {
                "timestamp": analysis_result.timestamp,
                "quality_score": analysis_result.quality_metrics.get('overall_score', 0),
                "total_issues": analysis_result.quality_metrics.get('issue_count', 0),
                "critical_issues": analysis_result.quality_metrics.get('critical_issues', 0),
                "recommendations_count": len(analysis_result.integrated_recommendations),
                "top_recommendations": analysis_result.integrated_recommendations[:5]
            },
            "detailed_analysis": {
                "global_analysis": analysis_result.global_analysis_results is not None,
                "detail_analysis": analysis_result.detail_analysis_results is not None,
                "all_recommendations": analysis_result.integrated_recommendations
            }
        }

        return [TextContent(
            type="text",
            text=json.dumps(result, ensure_ascii=False, indent=2)
        )]

    except Exception as e:
        import traceback
        error_details = traceback.format_exc()
        return [TextContent(
            type="text",
            text=json.dumps({
                "success": False,
                "error": str(e),
                "error_details": error_details
            }, ensure_ascii=False, indent=2)
        )]


async def build_and_analyze_kg_tool(arguments: dict[str, Any]) -> list[TextContent]:
    """
    ä¸€ä½“åŒ–å·¥å…·ï¼šæ„å»ºçŸ¥è¯†å›¾è°±ã€åˆ†æã€è‡ªåŠ¨å¢å¼ºå¹¶ç”Ÿæˆå¯è§†åŒ–
    """
    try:
        text = arguments.get("text", "")
        output_file = arguments.get("output_file", "enhanced_knowledge_graph.html")
        enable_analysis = arguments.get("enable_analysis", True)
        auto_enhance = arguments.get("auto_enhance", True)

        if not text.strip():
            return [TextContent(
                type="text",
                text=json.dumps({
                    "success": False,
                    "error": "è¾“å…¥æ–‡æœ¬ä¸èƒ½ä¸ºç©º"
                }, ensure_ascii=False, indent=2)
            )]

        start_time = time.time()

        # é˜¶æ®µ1ï¼šæ•°æ®è´¨é‡è¯„ä¼°
        quality_result = await quality_assessor.assess_quality(text)

        # é˜¶æ®µ2ï¼šçŸ¥è¯†è¡¥å…¨ï¼ˆå¦‚æœéœ€è¦ï¼‰
        processed_text = text
        completion_info = {"skipped": True, "reason": "æ•°æ®è´¨é‡è‰¯å¥½"}

        if not quality_result["is_high_quality"]:
            completion_result = await knowledge_completor.complete_knowledge(text, quality_result)
            processed_text = completion_result["enhanced_data"]
            completion_info = {
                "skipped": False,
                "completions": completion_result["completions"],
                "corrections": completion_result["corrections"],
                "confidence": completion_result["confidence"]
            }

        # é˜¶æ®µ3ï¼šçŸ¥è¯†å›¾è°±æ„å»º
        kg_result = await kg_builder.build_graph(processed_text, use_llm=True)

        if not kg_result["entities"] and not kg_result["triples"]:
            return [TextContent(
                type="text",
                text=json.dumps({
                    "success": False,
                    "error": "æ— æ³•ä»è¾“å…¥æ–‡æœ¬ä¸­æå–åˆ°æœ‰æ•ˆçš„å®ä½“æˆ–å…³ç³»"
                }, ensure_ascii=False, indent=2)
            )]

        # é˜¶æ®µ4ï¼šé«˜çº§åˆ†æï¼ˆå¦‚æœå¯ç”¨ï¼‰
        analysis_result = None
        if enable_analysis:
            # è½¬æ¢æ•°æ®æ ¼å¼ç”¨äºåˆ†æ
            entities = [
                {
                    'name': entity,
                    'type': 'unknown',
                    'attributes': {},
                    'relations': []
                }
                for entity in kg_result["entities"]
            ]

            relations = [
                {
                    'name': triple.relation,
                    'source': triple.head,
                    'target': triple.tail,
                    'type': 'unknown'
                }
                for triple in kg_result["triples"]
            ]

            # é…ç½®åˆ†æå‚æ•°
            config = AnalysisConfig(
                enable_global_analysis=True,
                enable_detail_analysis=True,
                similarity_threshold=0.3,
                max_recommendations=10
            )

            # æ‰§è¡Œé«˜çº§åˆ†æ
            analysis_result = await analyze_knowledge_graph(
                processed_text, entities, relations, config
            )

        # é˜¶æ®µ5ï¼šè‡ªåŠ¨å¢å¼ºï¼ˆå¦‚æœå¯ç”¨ï¼‰
        enhancement_result = None
        final_entities = kg_result["entities"]
        final_relations = kg_result["relations"]
        final_triples = kg_result["triples"]

        if auto_enhance and analysis_result:
            enhancement_result = await enhancement_executor.execute_enhancements(
                processed_text, kg_result["entities"], kg_result["relations"], kg_result["triples"], analysis_result
            )

            # ä½¿ç”¨å¢å¼ºåçš„æ•°æ®
            final_entities = [e['name'] for e in enhancement_result.enhanced_entities]
            final_relations = [r['name'] for r in enhancement_result.enhanced_relations]

            # æ„å»ºå¢å¼ºåçš„ä¸‰å…ƒç»„ç”¨äºå¯è§†åŒ–
            enhanced_triples = []
            for triple_dict in enhancement_result.enhanced_triples:
                from kg_utils import Triple
                enhanced_triple = Triple(
                    head=triple_dict['head'],
                    relation=triple_dict['relation'],
                    tail=triple_dict['tail'],
                    confidence=triple_dict.get('confidence', 0.8)
                )
                enhanced_triples.append(enhanced_triple)

            final_triples = enhanced_triples

        # é˜¶æ®µ6ï¼šç”Ÿæˆå¯è§†åŒ–
        visualization_file = kg_visualizer.save_simple_visualization(
            final_triples,
            final_entities,
            final_relations,
            output_file
        )

        abs_path = os.path.abspath(visualization_file)
        visualization_url = f"file:///{abs_path.replace(os.sep, '/')}"
        http_url = f"http://localhost:8000/{visualization_file}"

        processing_time = time.time() - start_time

        # æ„å»ºç»“æœ
        result = {
            "success": True,
            "input_text": text,
            "processing_time": round(processing_time, 3),
            "stages": {
                "quality_assessment": {
                    "quality_score": quality_result["quality_score"],
                    "is_high_quality": quality_result["is_high_quality"],
                    "completeness": quality_result["completeness"],
                    "consistency": quality_result["consistency"],
                    "relevance": quality_result["relevance"],
                    "issues": quality_result["issues"],
                    "recommendation": quality_result["recommendation"]
                },
                "knowledge_completion": completion_info,
                "original_knowledge_graph": {
                    "entities_count": len(kg_result["entities"]),
                    "relations_count": len(kg_result["relations"]),
                    "triples_count": len(kg_result["triples"]),
                    "entities": kg_result["entities"],
                    "relations": kg_result["relations"],
                    "average_confidence": sum(kg_result["confidence_scores"]) / len(kg_result["confidence_scores"]) if kg_result["confidence_scores"] else 0
                },
                "analysis_results": {
                    "analysis_enabled": enable_analysis,
                    "analysis_performed": analysis_result is not None,
                    "timestamp": analysis_result.timestamp if analysis_result else None,
                    "quality_score": analysis_result.quality_metrics.get('overall_score', 0) if analysis_result else None,
                    "total_issues": analysis_result.quality_metrics.get('issue_count', 0) if analysis_result else 0,
                    "critical_issues": analysis_result.quality_metrics.get('critical_issues', 0) if analysis_result else 0,
                    "recommendations_count": len(analysis_result.integrated_recommendations) if analysis_result else 0,
                    "top_recommendations": analysis_result.integrated_recommendations[:5] if analysis_result else []
                },
                "enhancement_results": {
                    "auto_enhance_enabled": auto_enhance,
                    "enhancement_applied": enhancement_result is not None,
                    "enhancement_summary": enhancement_result.enhancement_summary if enhancement_result else None,
                    "applied_enhancements": enhancement_result.applied_enhancements if enhancement_result else [],
                    "final_entities_count": len(final_entities),
                    "final_relations_count": len(final_relations),
                    "final_triples_count": len(final_triples)
                },
                "visualization": {
                    "file_path": visualization_file,
                    "file_url": visualization_url,
                    "http_url": http_url,
                    "server_info": f"å¯æ‰‹åŠ¨å¯åŠ¨HTTPæœåŠ¡å™¨è®¿é—®ï¼šåœ¨é¡¹ç›®ç›®å½•è¿è¡Œ 'python -m http.server 8000'ï¼Œç„¶åè®¿é—® {http_url}"
                }
            },
            "summary": {
                "original_text": text,
                "processed_text": processed_text,
                "quality_improved": not quality_result["is_high_quality"],
                "analysis_performed": analysis_result is not None,
                "enhancement_applied": enhancement_result is not None,
                "final_entities": len(final_entities),
                "final_relations": len(final_relations),
                "final_triples": len(final_triples),
                "visualization_ready": True,
                "visualization_file": visualization_file,
                "visualization_url": visualization_url
            }
        }

        return [TextContent(
            type="text",
            text=json.dumps(result, ensure_ascii=False, indent=2)
        )]

    except Exception as e:
        import traceback
        error_details = traceback.format_exc()
        return [TextContent(
            type="text",
            text=json.dumps({
                "success": False,
                "error": str(e),
                "error_details": error_details
            }, ensure_ascii=False, indent=2)
        )]


async def main():
    """
    è¿è¡ŒæœåŠ¡å™¨
    """
    print("ğŸš€ å¯åŠ¨çŸ¥è¯†å›¾è°±æ„å»ºæœåŠ¡å™¨ï¼ˆå¢å¼ºç‰ˆï¼‰")
    print(f"ğŸ”§ é«˜çº§åˆ†æåŠŸèƒ½: {'âœ… å¯ç”¨' if ANALYSIS_AVAILABLE else 'âŒ ä¸å¯ç”¨'}")
    
    try:
        # ç¡®ä¿æ‰€æœ‰ç»„ä»¶éƒ½æ­£å¸¸åˆå§‹åŒ–
        print("ğŸ”§ éªŒè¯ç»„ä»¶çŠ¶æ€...")
        if not hasattr(quality_assessor, 'assess_quality'):
            raise RuntimeError("è´¨é‡è¯„ä¼°å™¨æœªæ­£ç¡®åˆå§‹åŒ–")
        if not hasattr(kg_builder, 'build_graph'):
            raise RuntimeError("çŸ¥è¯†å›¾è°±æ„å»ºå™¨æœªæ­£ç¡®åˆå§‹åŒ–")
        print("âœ… æ‰€æœ‰ç»„ä»¶éªŒè¯é€šè¿‡")
        
        # ä½¿ç”¨ stdio ä¼ è¾“è¿è¡ŒæœåŠ¡å™¨
        print("ğŸ”— å¯åŠ¨MCPæœåŠ¡å™¨...")
        
        # åˆ›å»ºåˆå§‹åŒ–é€‰é¡¹
        init_options = InitializationOptions(
            server_name="knowledge-graph-builder-enhanced",
            server_version="2.0.0",
            capabilities=server.get_capabilities(
                notification_options=NotificationOptions(),
                experimental_capabilities={}
            ),
        )
        
        async with stdio_server() as (read_stream, write_stream):
            # ç¡®ä¿æœåŠ¡å™¨å®Œå…¨åˆå§‹åŒ–
            print("â³ ç­‰å¾…æœåŠ¡å™¨å®Œå…¨åˆå§‹åŒ–...")
            await asyncio.sleep(0.5)  # å¢åŠ å»¶è¿Ÿç¡®ä¿åˆå§‹åŒ–å®Œæˆ
            
            print("âœ… å¼€å§‹è¿è¡ŒæœåŠ¡å™¨...")
            await server.run(
                read_stream,
                write_stream,
                init_options,
            )
            
    except KeyboardInterrupt:
        print("\nğŸ›‘ æœåŠ¡å™¨æ”¶åˆ°ä¸­æ–­ä¿¡å·ï¼Œæ­£åœ¨å…³é—­...")
    except Exception as e:
        print(f"âŒ æœåŠ¡å™¨å¯åŠ¨å¤±è´¥: {e}")
        import traceback
        traceback.print_exc()
        raise
    finally:
        print("ğŸ”š æœåŠ¡å™¨å·²å…³é—­")


if __name__ == "__main__":
    asyncio.run(main()) 