# kg_utils.py - çº¯LLMç‰ˆæœ¬ï¼ˆç§»é™¤æ‰€æœ‰ç¡¬ç¼–ç ï¼‰

import json
import re
import asyncio
import requests
from typing import Dict, List, Tuple, Any, Optional
from dataclasses import dataclass
from collections import defaultdict


@dataclass
class Triple:
    """çŸ¥è¯†å›¾è°±ä¸‰å…ƒç»„"""
    head: str
    relation: str
    tail: str
    confidence: float = 1.0

    def to_dict(self) -> Dict[str, Any]:
        return {
            "head": self.head,
            "relation": self.relation,
            "tail": self.tail,
            "confidence": self.confidence
        }

    def __str__(self) -> str:
        return f"({self.head}, {self.relation}, {self.tail})"


class ChineseEntityRelationExtractor:
    """çº¯LLMä¸­æ–‡å®ä½“æŠ½å–ä¸å…³ç³»æŠ½å–"""
    
    def __init__(self, api_key):
        self.api_key = api_key  # ä¿®å¤ï¼šä½¿ç”¨ä¼ å…¥çš„APIå¯†é’¥
        self.base_url = "https://api.siliconflow.cn/v1/chat/completions"
        self.model = "Qwen/Qwen2.5-7B-Instruct"
        
    def call_api(self, prompt):
        """è°ƒç”¨Silicon Flow API"""
        headers = {
            "Authorization": f"Bearer {self.api_key}",
            "Content-Type": "application/json"
        }
        
        data = {
            "model": self.model,
            "messages": [{"role": "user", "content": prompt}],
            "temperature": 0.1,
            "max_tokens": 800
        }
        
        try:
            response = requests.post(self.base_url, headers=headers, json=data, timeout=30)
            
            if response.status_code == 200:
                result = response.json()
                if "choices" in result and len(result["choices"]) > 0:
                    return result["choices"][0]["message"]["content"]
            else:
                print(f"âŒ APIè°ƒç”¨å¤±è´¥: HTTP {response.status_code}")
                return None
            
        except Exception as e:
            print(f"âŒ APIè°ƒç”¨å¤±è´¥: {e}")
            return None
    
    def extract_entities_and_types(self, text):
        """åŒæ—¶æå–å®ä½“å’Œç±»å‹"""
        prompt = f"""
è¯·ä»ä»¥ä¸‹ä¸­æ–‡æ–‡æœ¬ä¸­æå–å®ä½“ï¼Œå¹¶æ ‡æ³¨æ¯ä¸ªå®ä½“çš„ç±»å‹ã€‚

æ–‡æœ¬ï¼š"{text}"

è¯·æŒ‰ç…§ä»¥ä¸‹æ ¼å¼è¾“å‡ºï¼Œæ¯è¡Œä¸€ä¸ªå®ä½“ï¼š
å®ä½“åç§°|å®ä½“ç±»å‹

å®ä½“ç±»å‹åŒ…æ‹¬ï¼šPersonï¼ˆäººç‰©ï¼‰ã€Organizationï¼ˆç»„ç»‡ï¼‰ã€Locationï¼ˆåœ°ç‚¹ï¼‰ã€Productï¼ˆäº§å“ï¼‰ã€Eventï¼ˆäº‹ä»¶ï¼‰ã€Otherï¼ˆå…¶ä»–ï¼‰

ç¤ºä¾‹æ ¼å¼ï¼š
å¼ ä¸‰|Person
é˜¿é‡Œå·´å·´|Organization
åŒ—äº¬|Location
iPhone|Product
"""
        
        response = self.call_api(prompt)
        if response:
            entities = {}
            lines = response.strip().split('\n')
            for line in lines:
                if '|' in line:
                    parts = line.split('|')
                    if len(parts) >= 2:
                        entity = parts[0].strip()
                        entity_type = parts[1].strip()
                        if entity:
                            entities[entity] = entity_type
            return entities
        return {}

    def extract_triplets(self, text):
        """æå–ä¸‰å…ƒç»„"""
        prompt = f"""
è¯·ä»ä»¥ä¸‹ä¸­æ–‡æ–‡æœ¬ä¸­æŠ½å–å®ä½“å…³ç³»ä¸‰å…ƒç»„ã€‚

æ–‡æœ¬ï¼š"{text}"

è¯·ä¸¥æ ¼æŒ‰ç…§ä»¥ä¸‹æ ¼å¼è¾“å‡ºï¼Œæ¯è¡Œä¸€ä¸ªä¸‰å…ƒç»„ï¼š
(å¤´å®ä½“,å…³ç³»,å°¾å®ä½“)

è¦æ±‚ï¼š
1. å¤´å®ä½“å’Œå°¾å®ä½“å¿…é¡»æ˜¯æ–‡æœ¬ä¸­æ˜ç¡®å‡ºç°çš„
2. å…³ç³»è¦å‡†ç¡®æè¿°ä¸¤ä¸ªå®ä½“ä¹‹é—´çš„å…³ç³»
3. åªè¾“å‡ºç¡®å®šçš„å…³ç³»ï¼Œä¸è¦çŒœæµ‹
4. æ¯è¡Œåªè¾“å‡ºä¸€ä¸ªä¸‰å…ƒç»„

ç¤ºä¾‹ï¼š
(å¼ ä¸‰,æ‹…ä»»,CEO)
(é˜¿é‡Œå·´å·´,æ€»éƒ¨ä½äº,æ­å·)
"""
        
        response = self.call_api(prompt)
        if response:
            return self.parse_triplets(response)
        return []
    
    def parse_triplets(self, response):
        """è§£æä¸‰å…ƒç»„"""
        triplets = []
        lines = response.strip().split('\n')
        
        for line in lines:
            # åŒ¹é…æ ¼å¼ï¼š(å¤´å®ä½“,å…³ç³»,å°¾å®ä½“)
            match = re.search(r'\(([^,]+),\s*([^,]+),\s*([^)]+)\)', line)
            if match:
                head = match.group(1).strip()
                relation = match.group(2).strip()
                tail = match.group(3).strip()
                if head and relation and tail:
                    triplets.append((head, relation, tail))
        
        return triplets


class PureLLMKnowledgeGraphBuilder:
    """çº¯LLMçŸ¥è¯†å›¾è°±æ„å»ºå™¨ï¼ˆæ— ç¡¬ç¼–ç è§„åˆ™ï¼‰"""

    def __init__(self, api_key: Optional[str] = None):
        self.entities = set()
        self.relations = set()
        self.triples = []
        self.entity_types = {}
        
        # åˆå§‹åŒ–LLMæŠ½å–å™¨
        self.llm_extractor = None
        if api_key:
            self.llm_extractor = ChineseEntityRelationExtractor(api_key)
        else:
            print("âš ï¸  è­¦å‘Šï¼šæœªæä¾›APIå¯†é’¥ï¼Œæ— æ³•ä½¿ç”¨LLMåŠŸèƒ½")

    async def build_graph(self, data: str, use_llm: bool = True) -> Dict[str, Any]:
        """
        æ„å»ºçŸ¥è¯†å›¾è°±ï¼ˆçº¯LLMç‰ˆæœ¬ï¼‰
        
        Args:
            data: è¾“å…¥æ–‡æœ¬
            use_llm: å¿…é¡»ä¸ºTrueï¼Œçº¯LLMç‰ˆæœ¬ä¸æ”¯æŒè§„åˆ™æ¨¡å¼
        
        Returns:
            çŸ¥è¯†å›¾è°±æ„å»ºç»“æœ
        """
        if not use_llm:
            print("âš ï¸  çº¯LLMç‰ˆæœ¬ä¸æ”¯æŒè§„åˆ™æ¨¡å¼ï¼Œè‡ªåŠ¨å¯ç”¨LLMæ¨¡å¼")
        
        if not self.llm_extractor:
            print("âŒ æœªé…ç½®APIå¯†é’¥ï¼Œæ— æ³•æ„å»ºçŸ¥è¯†å›¾è°±")
            return {
                "entities": [],
                "relations": [],
                "triples": [],
                "confidence_scores": []
            }

        print("ğŸ¤– ä½¿ç”¨çº¯LLMæ¨¡å¼æ„å»ºçŸ¥è¯†å›¾è°±...")
        
        # ä½¿ç”¨LLMæå–æ‰€æœ‰ä¿¡æ¯
        entities_with_types = self.llm_extractor.extract_entities_and_types(data)
        llm_triplets = self.llm_extractor.extract_triplets(data)
        
        # å¤„ç†å®ä½“
        entities = list(entities_with_types.keys())
        self.entity_types = entities_with_types
        
        # å¤„ç†ä¸‰å…ƒç»„
        triples = []
        relations = set()
        
        for head, relation, tail in llm_triplets:
            # è®¡ç®—ç½®ä¿¡åº¦
            confidence = self._calculate_llm_confidence(data, head, relation, tail)
            triple = Triple(head, relation, tail, confidence)
            triples.append(triple)
            relations.add(relation)
            
            # ç¡®ä¿å®ä½“è¢«åŒ…å«
            if head not in entities:
                entities.append(head)
                self.entity_types[head] = "Other"
            if tail not in entities:
                entities.append(tail)
                self.entity_types[tail] = "Other"

        # å»é‡å’Œåˆå¹¶
        triples = self._merge_duplicate_triples(triples)
        
        # è®¡ç®—ç½®ä¿¡åº¦
        confidence_scores = [triple.confidence for triple in triples]

        # æ›´æ–°å†…éƒ¨çŠ¶æ€
        self.entities.update(entities)
        self.relations.update(relations)
        self.triples.extend(triples)

        print(f"âœ… æå–å®Œæˆï¼š{len(entities)}ä¸ªå®ä½“ï¼Œ{len(relations)}ä¸ªå…³ç³»ï¼Œ{len(triples)}ä¸ªä¸‰å…ƒç»„")

        return {
            "entities": entities,
            "relations": list(relations),
            "triples": triples,
            "confidence_scores": confidence_scores
        }

    def _calculate_llm_confidence(self, data: str, head: str, relation: str, tail: str) -> float:
        """è®¡ç®—LLMä¸‰å…ƒç»„ç½®ä¿¡åº¦ - åŠ¨æ€è®¡ç®—ï¼Œé¿å…ç¡¬ç¼–ç """
        confidence = 0.5  # åŸºç¡€ç½®ä¿¡åº¦é™ä½
        
        # 1. å®ä½“åœ¨åŸæ–‡ä¸­çš„ä½ç½®å’Œé¢‘ç‡
        head_count = data.count(head)
        tail_count = data.count(tail)
        
        # å®ä½“å‡ºç°é¢‘ç‡å½±å“ç½®ä¿¡åº¦
        if head_count > 0 and tail_count > 0:
            frequency_bonus = min(0.3, (head_count + tail_count) * 0.05)
            confidence += frequency_bonus
        
        # 2. å®ä½“è·ç¦»åˆ†æ
        head_pos = data.find(head)
        tail_pos = data.find(tail)
        if head_pos != -1 and tail_pos != -1:
            distance = abs(head_pos - tail_pos)
            text_length = len(data)
            # è·ç¦»è¶Šè¿‘ï¼Œç½®ä¿¡åº¦è¶Šé«˜
            proximity_bonus = max(0, 0.2 * (1 - distance / text_length))
            confidence += proximity_bonus
        
        # 3. å…³ç³»è¯åœ¨å®ä½“é™„è¿‘çš„å­˜åœ¨
        relation_in_context = False
        if head_pos != -1 and tail_pos != -1:
            start_pos = min(head_pos, tail_pos)
            end_pos = max(head_pos, tail_pos) + max(len(head), len(tail))
            context = data[max(0, start_pos-20):min(len(data), end_pos+20)]
            
            # æ£€æŸ¥å…³ç³»è¯æˆ–ç›¸å…³è¯æ˜¯å¦åœ¨ä¸Šä¸‹æ–‡ä¸­
            relation_words = [relation, 'æ˜¯', 'åœ¨', 'çš„', 'æœ‰', 'å±äº', 'æ‹…ä»»', 'ä½äº']
            if any(word in context for word in relation_words):
                confidence += 0.15
                relation_in_context = True
        
        # 4. å®ä½“ç±»å‹åŒ¹é…åº¦
        head_type = self.entity_types.get(head, "Other")
        tail_type = self.entity_types.get(tail, "Other")
        
        # åˆç†çš„ç±»å‹ç»„åˆè·å¾—æ›´é«˜ç½®ä¿¡åº¦
        type_combinations = {
            ("Person", "Organization"): 0.15,
            ("Person", "Location"): 0.12,
            ("Organization", "Location"): 0.10,
            ("Person", "Product"): 0.08,
            ("Organization", "Product"): 0.08,
        }
        
        combination_key = (head_type, tail_type)
        reverse_key = (tail_type, head_type)
        
        if combination_key in type_combinations:
            confidence += type_combinations[combination_key]
        elif reverse_key in type_combinations:
            confidence += type_combinations[reverse_key]
        
        # 5. å…³ç³»ç±»å‹çš„å¯ä¿¡åº¦
        relation_confidence_map = {
            'æ˜¯': 0.9, 'æ‹…ä»»': 0.85, 'ä½äº': 0.85, 'å±äº': 0.8,
            'å·¥ä½œäº': 0.8, 'å°±èŒäº': 0.8, 'æ¯•ä¸šäº': 0.75, 'å­¦ä¹ ': 0.7,
            'å¼€å‘': 0.7, 'åˆ¶ä½œ': 0.7, 'åˆ›å»º': 0.7, 'æ‹¥æœ‰': 0.65,
            'åŒ…å«': 0.6, 'ç›¸å…³': 0.5, 'å…³è”': 0.5
        }
        
        relation_base = relation_confidence_map.get(relation, 0.6)
        confidence = confidence * relation_base
        
        # 6. æ–‡æœ¬é•¿åº¦å½±å“ï¼ˆè¾ƒé•¿æ–‡æœ¬å¯èƒ½åŒ…å«æ›´å¤šå™ªéŸ³ï¼‰
        text_length = len(data)
        if text_length > 500:
            confidence *= 0.95  # è½»å¾®é™ä½
        elif text_length < 50:
            confidence *= 0.9   # æ–‡æœ¬å¤ªçŸ­å¯èƒ½ä¿¡æ¯ä¸è¶³
        
        # 7. ç¡®ä¿ç½®ä¿¡åº¦åœ¨åˆç†èŒƒå›´å†…
        confidence = max(0.1, min(0.98, confidence))
        
        # 8. æ·»åŠ ä¸€äº›éšæœºæ€§é¿å…å®Œå…¨ç›¸åŒçš„ç½®ä¿¡åº¦
        import random
        random.seed(hash(head + relation + tail) % 1000)  # åŸºäºå†…å®¹çš„ä¼ªéšæœº
        noise = (random.random() - 0.5) * 0.05  # Â±2.5%çš„å™ªå£°
        confidence += noise
        
        return round(max(0.1, min(0.98, confidence)), 3)

    def _merge_duplicate_triples(self, triples: List[Triple]) -> List[Triple]:
        """åˆå¹¶é‡å¤ä¸‰å…ƒç»„"""
        triple_dict = {}
        
        for triple in triples:
            key = (triple.head, triple.relation, triple.tail)
            if key in triple_dict:
                # ä¿ç•™ç½®ä¿¡åº¦æ›´é«˜çš„
                if triple.confidence > triple_dict[key].confidence:
                    triple_dict[key] = triple
            else:
                triple_dict[key] = triple
        
        return list(triple_dict.values())

    def get_statistics(self) -> Dict[str, Any]:
        """è·å–ç»Ÿè®¡ä¿¡æ¯"""
        entity_type_counts = defaultdict(int)
        for entity, entity_type in self.entity_types.items():
            entity_type_counts[entity_type] += 1

        relation_counts = defaultdict(int)
        for triple in self.triples:
            relation_counts[triple.relation] += 1

        return {
            "total_entities": len(self.entities),
            "total_relations": len(self.relations),
            "total_triples": len(self.triples),
            "entity_types": dict(entity_type_counts),
            "relation_distribution": dict(relation_counts),
            "average_confidence": sum(triple.confidence for triple in self.triples) / len(self.triples) if self.triples else 0
        }

    def export_graph(self, format_type: str = "json") -> str:
        """å¯¼å‡ºçŸ¥è¯†å›¾è°±"""
        if format_type == "json":
            graph_data = {
                "entities": [{"id": entity, "type": self.entity_types.get(entity, "Other")} for entity in self.entities],
                "relations": list(self.relations),
                "triples": [triple.to_dict() for triple in self.triples]
            }
            return json.dumps(graph_data, ensure_ascii=False, indent=2)
        else:
            raise ValueError(f"ä¸æ”¯æŒçš„æ ¼å¼: {format_type}")

    def clear_graph(self):
        """æ¸…ç©ºçŸ¥è¯†å›¾è°±"""
        self.entities.clear()
        self.relations.clear()
        self.triples.clear()
        self.entity_types.clear()


# ä¸ºäº†å…¼å®¹æ€§ï¼Œä¿ç•™åŸæ¥çš„ç±»å
class KnowledgeGraphBuilder(PureLLMKnowledgeGraphBuilder):
    """çŸ¥è¯†å›¾è°±æ„å»ºå™¨ï¼ˆå…¼å®¹æ€§åˆ«åï¼‰"""
    pass