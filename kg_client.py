import json
import asyncio
import os
import webbrowser
from typing import Optional
from contextlib import AsyncExitStack

from openai import OpenAI
from dotenv import load_dotenv

from mcp import ClientSession, StdioServerParameters
from mcp.client.stdio import stdio_client

load_dotenv()


class KnowledgeGraphClient:
    def __init__(self):
        self.session: Optional[ClientSession] = None
        self.exit_stack = AsyncExitStack()
        self.client = OpenAI(
            api_key=os.getenv("OPENAI_API_KEY"),
            base_url=os.getenv("OPENAI_BASE_URL")
        )

    async def connect_to_server(self):
        """è¿æ¥åˆ°çŸ¥è¯†å›¾è°±æœåŠ¡å™¨"""
        server_params = StdioServerParameters(
            command='uv',
            args=['run', 'kg_server.py'],
            env=None
        )

        stdio_transport = await self.exit_stack.enter_async_context(
            stdio_client(server_params))
        stdio, write = stdio_transport
        self.session = await self.exit_stack.enter_async_context(
            ClientSession(stdio, write))

        await self.session.initialize()

    async def build_knowledge_graph(self, text: str, output_file: str = "knowledge_graph.html") -> dict:
        """
        æ„å»ºçŸ¥è¯†å›¾è°±

        Args:
            text: è¦å¤„ç†çš„æ–‡æœ¬æ•°æ®
            output_file: å¯è§†åŒ–è¾“å‡ºæ–‡ä»¶å

        Returns:
            åŒ…å«çŸ¥è¯†å›¾è°±æ„å»ºç»“æœçš„å­—å…¸
        """
        try:
            # ç›´æ¥è°ƒç”¨çŸ¥è¯†å›¾è°±æ„å»ºå·¥å…·
            result = await self.session.call_tool("build_knowledge_graph", {
                "text": text,
                "output_file": output_file
            })

            # è§£æç»“æœ
            result_text = result.content[0].text
            result_data = json.loads(result_text)

            return result_data

        except Exception as e:
            return {
                "success": False,
                "error": f"æ„å»ºçŸ¥è¯†å›¾è°±æ—¶å‘ç”Ÿé”™è¯¯: {str(e)}"
            }

    async def process_text_with_ai(self, text: str) -> str:
        """
        ä½¿ç”¨AIå¤„ç†æ–‡æœ¬å¹¶æ„å»ºçŸ¥è¯†å›¾è°±

        Args:
            text: ç”¨æˆ·è¾“å…¥çš„æ–‡æœ¬

        Returns:
            AIç”Ÿæˆçš„å›ç­”
        """
        system_prompt = (
            "ä½ æ˜¯ä¸€ä¸ªçŸ¥è¯†å›¾è°±æ„å»ºåŠ©æ‰‹ã€‚"
            "ä½ å¯ä»¥å¸®åŠ©ç”¨æˆ·ä»æ–‡æœ¬ä¸­æ„å»ºçŸ¥è¯†å›¾è°±ã€‚"
            "å½“ç”¨æˆ·æä¾›æ–‡æœ¬æ—¶ï¼Œä½ å¿…é¡»è°ƒç”¨ build_knowledge_graph å·¥å…·æ¥æ„å»ºçŸ¥è¯†å›¾è°±ã€‚"
            "ç„¶ååŸºäºæ„å»ºç»“æœä¸ºç”¨æˆ·æä¾›è¯¦ç»†çš„åˆ†æå’Œè¯´æ˜ã€‚"
        )

        messages = [
            {"role": "system", "content": system_prompt},
            {"role": "user", "content": text}
        ]

        # è·å–å·¥å…·ä¿¡æ¯
        response = await self.session.list_tools()
        available_tools = [{
            "type": "function",
            "function": {
                "name": tool.name,
                "description": tool.description,
                "parameters": tool.inputSchema
            }
        } for tool in response.tools]

        # è¯·æ±‚å¤§æ¨¡å‹
        response = self.client.chat.completions.create(
            model=os.getenv("OPENAI_MODEL"),
            messages=messages,
            tools=available_tools
        )

        content = response.choices[0]
        if content.finish_reason == "tool_calls":
            tool_call = content.message.tool_calls[0]
            tool_name = tool_call.function.name
            tool_args = json.loads(tool_call.function.arguments)

            result = await self.session.call_tool(tool_name, tool_args)

            messages.append(content.message.model_dump())
            messages.append({
                "role": "tool",
                "content": result.content[0].text,
                "tool_call_id": tool_call.id,
            })

            # ç»§ç»­å‘å¤§æ¨¡å‹å‘é€è¯·æ±‚ä»¥ç”Ÿæˆæœ€ç»ˆå›ç­”
            response = self.client.chat.completions.create(
                model=os.getenv("OPENAI_MODEL"),
                messages=messages
            )
            return response.choices[0].message.content

        return content.message.content

    def display_result(self, result: dict):
        """æ˜¾ç¤ºçŸ¥è¯†å›¾è°±æ„å»ºç»“æœ"""
        if not result.get("success", False):
            print(f"\nâŒ é”™è¯¯: {result.get('error', 'æœªçŸ¥é”™è¯¯')}")
            return

        print("\nâœ… çŸ¥è¯†å›¾è°±æ„å»ºæˆåŠŸ!")
        print(f"â±ï¸  å¤„ç†æ—¶é—´: {result.get('processing_time', 0):.3f} ç§’")

        # æ˜¾ç¤ºé˜¶æ®µä¿¡æ¯
        stages = result.get("stages", {})

        # æ•°æ®è´¨é‡è¯„ä¼°
        quality = stages.get("quality_assessment", {})
        print(f"\nğŸ“Š æ•°æ®è´¨é‡è¯„ä¼°:")
        print(f"   è´¨é‡åˆ†æ•°: {quality.get('quality_score', 0):.3f}")
        print(f"   é«˜è´¨é‡æ•°æ®: {'æ˜¯' if quality.get('is_high_quality', False) else 'å¦'}")
        print(f"   å®Œæ•´æ€§: {quality.get('completeness', 0):.3f}")
        print(f"   ä¸€è‡´æ€§: {quality.get('consistency', 0):.3f}")
        print(f"   ç›¸å…³æ€§: {quality.get('relevance', 0):.3f}")

        # çŸ¥è¯†è¡¥å…¨
        completion = stages.get("knowledge_completion", {})
        if not completion.get("skipped", True):
            print(f"\nğŸ”§ çŸ¥è¯†è¡¥å…¨:")
            print(f"   ç½®ä¿¡åº¦: {completion.get('confidence', 0):.3f}")
            print(f"   è¡¥å…¨æ•°é‡: {len(completion.get('completions', []))}")
            print(f"   ä¿®æ­£æ•°é‡: {len(completion.get('corrections', []))}")
        else:
            print(f"\nğŸ”§ çŸ¥è¯†è¡¥å…¨: è·³è¿‡ (æ•°æ®è´¨é‡è‰¯å¥½)")

        # çŸ¥è¯†å›¾è°±
        kg = stages.get("knowledge_graph", {})
        print(f"\nğŸ•¸ï¸  çŸ¥è¯†å›¾è°±:")
        print(f"   å®ä½“æ•°é‡: {kg.get('entities_count', 0)}")
        print(f"   å…³ç³»ç±»å‹: {kg.get('relations_count', 0)}")
        print(f"   ä¸‰å…ƒç»„æ•°é‡: {kg.get('triples_count', 0)}")
        print(f"   å¹³å‡ç½®ä¿¡åº¦: {kg.get('average_confidence', 0):.3f}")

        # å¯è§†åŒ–
        viz = stages.get("visualization", {})
        print(f"\nğŸ¨ å¯è§†åŒ–:")
        print(f"   æ–‡ä»¶è·¯å¾„: {viz.get('file_path', 'N/A')}")
        print(f"   æ–‡ä»¶å¤§å°: {viz.get('file_size', 0)} å­—èŠ‚")

        # å°è¯•æ‰“å¼€å¯è§†åŒ–æ–‡ä»¶
        file_url = viz.get("file_url", "")
        if file_url and os.path.exists(viz.get("file_path", "")):
            try:
                print(f"\nğŸŒ æ­£åœ¨æ‰“å¼€å¯è§†åŒ–æ–‡ä»¶...")
                webbrowser.open(file_url)
                print(f"   å·²åœ¨æµè§ˆå™¨ä¸­æ‰“å¼€: {file_url}")
            except Exception as e:
                print(f"   æ— æ³•è‡ªåŠ¨æ‰“å¼€æµè§ˆå™¨: {e}")
                print(f"   è¯·æ‰‹åŠ¨æ‰“å¼€: {file_url}")

        print(f"\nğŸ’¡ æç¤º: {viz.get('server_info', '')}")

    async def interactive_mode(self):
        """äº¤äº’å¼æ¨¡å¼"""
        print("ğŸ¯ çŸ¥è¯†å›¾è°±æ„å»ºå®¢æˆ·ç«¯")
        print("è¾“å…¥æ–‡æœ¬æ¥æ„å»ºçŸ¥è¯†å›¾è°±ï¼Œè¾“å…¥ 'quit' é€€å‡º")
        print("=" * 50)

        while True:
            try:
                text = input("\nğŸ“ è¯·è¾“å…¥æ–‡æœ¬: ").strip()
                if text.lower() == 'quit':
                    break

                if not text:
                    print("âŒ è¯·è¾“å…¥æœ‰æ•ˆçš„æ–‡æœ¬")
                    continue

                print("\nğŸ”„ æ­£åœ¨æ„å»ºçŸ¥è¯†å›¾è°±...")
                result = await self.build_knowledge_graph(text)
                self.display_result(result)

            except KeyboardInterrupt:
                print("\n\nğŸ‘‹ å†è§!")
                break
            except Exception as e:
                print(f"\nâŒ å‘ç”Ÿé”™è¯¯: {e}")
                import traceback
                traceback.print_exc()

    async def batch_mode(self, texts: list, output_prefix: str = "kg"):
        """æ‰¹é‡å¤„ç†æ¨¡å¼"""
        print(f"ğŸ”„ æ‰¹é‡å¤„ç† {len(texts)} ä¸ªæ–‡æœ¬...")

        for i, text in enumerate(texts, 1):
            print(f"\nå¤„ç†ç¬¬ {i}/{len(texts)} ä¸ªæ–‡æœ¬...")
            output_file = f"{output_prefix}_{i}.html"
            result = await self.build_knowledge_graph(text, output_file)

            if result.get("success"):
                print(f"âœ… ç¬¬ {i} ä¸ªæ–‡æœ¬å¤„ç†å®Œæˆ: {output_file}")
            else:
                print(f"âŒ ç¬¬ {i} ä¸ªæ–‡æœ¬å¤„ç†å¤±è´¥: {result.get('error')}")

    async def cleanup(self):
        await self.exit_stack.aclose()


async def main():
    """ä¸»å‡½æ•°"""
    client = KnowledgeGraphClient()
    try:
        print("ğŸ”— æ­£åœ¨è¿æ¥åˆ°çŸ¥è¯†å›¾è°±æœåŠ¡å™¨...")
        await client.connect_to_server()
        print("âœ… è¿æ¥æˆåŠŸ!")

        # æ£€æŸ¥å‘½ä»¤è¡Œå‚æ•°
        import sys
        if len(sys.argv) > 1:
            # æ‰¹é‡æ¨¡å¼
            texts = sys.argv[1:]
            await client.batch_mode(texts)
        else:
            # äº¤äº’å¼æ¨¡å¼
            await client.interactive_mode()

    except Exception as e:
        print(f"âŒ è¿æ¥å¤±è´¥: {e}")
        import traceback
        traceback.print_exc()
    finally:
        await client.cleanup()


if __name__ == "__main__":
    asyncio.run(main())
